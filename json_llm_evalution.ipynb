{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cefcbf49-1df4-4a75-a613-7b04c934a8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running batch evaluation...\n",
      "Starting lenient evaluation: json_files/16.json\n",
      "Lenient evaluation completed - Average Score: 78.3%\n",
      "Starting lenient evaluation: json_files/6.json\n",
      "Lenient evaluation completed - Average Score: 86.7%\n",
      "Starting lenient evaluation: json_files/7.json\n",
      "Lenient evaluation completed - Average Score: 90.0%\n",
      "Starting lenient evaluation: json_files/10.json\n",
      "Lenient evaluation completed - Average Score: 87.3%\n",
      "Starting lenient evaluation: json_files/1.json\n",
      "Lenient evaluation completed - Average Score: 78.3%\n",
      "Starting lenient evaluation: json_files/2.json\n",
      "Lenient evaluation completed - Average Score: 80.0%\n",
      "Starting lenient evaluation: json_files/12.json\n",
      "Lenient evaluation completed - Average Score: 90.0%\n",
      "Starting lenient evaluation: json_files/13.json\n",
      "Lenient evaluation completed - Average Score: 76.7%\n",
      "Starting lenient evaluation: json_files/3.json\n",
      "Lenient evaluation completed - Average Score: 90.0%\n",
      "Starting lenient evaluation: json_files/18.json\n",
      "Lenient evaluation completed - Average Score: 80.0%\n",
      "Starting lenient evaluation: json_files/4.json\n",
      "Lenient evaluation completed - Average Score: 86.7%\n",
      "Warning: No markdown file found for 17_model2.json\n",
      "Starting lenient evaluation: json_files/15.json\n",
      "Lenient evaluation completed - Average Score: 90.0%\n",
      "Starting lenient evaluation: json_files/5.json\n",
      "Lenient evaluation completed - Average Score: 78.3%\n",
      "Starting lenient evaluation: json_files/19.json\n",
      "Lenient evaluation completed - Average Score: 86.7%\n",
      "Starting lenient evaluation: json_files/9.json\n",
      "Lenient evaluation completed - Average Score: 86.7%\n",
      "\n",
      "==================================================\n",
      "INDIVIDUAL FILE RESULTS\n",
      "==================================================\n",
      "\n",
      "File: 16.json\n",
      "Average Score: 78.3%\n",
      "  Overall Accuracy: 75% (conf: 3)\n",
      "  Semantic Fidelity: 70% (conf: 3)\n",
      "  Format Compliance: 90% (conf: 4)\n",
      "\n",
      "File: 6.json\n",
      "Average Score: 86.7%\n",
      "  Overall Accuracy: 85% (conf: 4)\n",
      "  Semantic Fidelity: 80% (conf: 4)\n",
      "  Format Compliance: 95% (conf: 5)\n",
      "\n",
      "File: 7.json\n",
      "Average Score: 90.0%\n",
      "  Overall Accuracy: 85% (conf: 4)\n",
      "  Semantic Fidelity: 90% (conf: 5)\n",
      "  Format Compliance: 95% (conf: 5)\n",
      "\n",
      "File: 10.json\n",
      "Average Score: 87.3%\n",
      "  Overall Accuracy: 85% (conf: 4)\n",
      "  Semantic Fidelity: 82% (conf: 4)\n",
      "  Format Compliance: 95% (conf: 5)\n",
      "\n",
      "File: 1.json\n",
      "Average Score: 78.3%\n",
      "  Overall Accuracy: 75% (conf: 4)\n",
      "  Semantic Fidelity: 65% (conf: 3)\n",
      "  Format Compliance: 95% (conf: 5)\n",
      "\n",
      "File: 2.json\n",
      "Average Score: 80.0%\n",
      "  Overall Accuracy: 75% (conf: 4)\n",
      "  Semantic Fidelity: 70% (conf: 3)\n",
      "  Format Compliance: 95% (conf: 5)\n",
      "\n",
      "File: 12.json\n",
      "Average Score: 90.0%\n",
      "  Overall Accuracy: 85% (conf: 4)\n",
      "  Semantic Fidelity: 90% (conf: 4)\n",
      "  Format Compliance: 95% (conf: 5)\n",
      "\n",
      "File: 13.json\n",
      "Average Score: 76.7%\n",
      "  Overall Accuracy: 75% (conf: 3)\n",
      "  Semantic Fidelity: 70% (conf: 3)\n",
      "  Format Compliance: 85% (conf: 4)\n",
      "\n",
      "File: 3.json\n",
      "Average Score: 90.0%\n",
      "  Overall Accuracy: 85% (conf: 4)\n",
      "  Semantic Fidelity: 90% (conf: 4)\n",
      "  Format Compliance: 95% (conf: 5)\n",
      "\n",
      "File: 18.json\n",
      "Average Score: 80.0%\n",
      "  Overall Accuracy: 75% (conf: 4)\n",
      "  Semantic Fidelity: 70% (conf: 3)\n",
      "  Format Compliance: 95% (conf: 5)\n",
      "\n",
      "File: 4.json\n",
      "Average Score: 86.7%\n",
      "  Overall Accuracy: 85% (conf: 4)\n",
      "  Semantic Fidelity: 80% (conf: 4)\n",
      "  Format Compliance: 95% (conf: 5)\n",
      "\n",
      "File: 17_model2.json\n",
      "Average Score: 50.0%\n",
      "  Overall Accuracy: 0% (conf: 0)\n",
      "  Semantic Fidelity: 0% (conf: 0)\n",
      "  Format Compliance: 0% (conf: 0)\n",
      "\n",
      "File: 15.json\n",
      "Average Score: 90.0%\n",
      "  Overall Accuracy: 85% (conf: 4)\n",
      "  Semantic Fidelity: 90% (conf: 5)\n",
      "  Format Compliance: 95% (conf: 5)\n",
      "\n",
      "File: 5.json\n",
      "Average Score: 78.3%\n",
      "  Overall Accuracy: 75% (conf: 4)\n",
      "  Semantic Fidelity: 70% (conf: 3)\n",
      "  Format Compliance: 90% (conf: 5)\n",
      "\n",
      "File: 19.json\n",
      "Average Score: 86.7%\n",
      "  Overall Accuracy: 85% (conf: 4)\n",
      "  Semantic Fidelity: 80% (conf: 4)\n",
      "  Format Compliance: 95% (conf: 5)\n",
      "\n",
      "File: 9.json\n",
      "Average Score: 86.7%\n",
      "  Overall Accuracy: 85% (conf: 4)\n",
      "  Semantic Fidelity: 80% (conf: 4)\n",
      "  Format Compliance: 95% (conf: 5)\n",
      "\n",
      "PERFORMANCE TABLE:\n",
      "\\begin{table}[htbp]\n",
      "  \\centering\n",
      "  \\small\n",
      "  \\caption{Lenient Performance Evaluation Results}\n",
      "  \\label{tab:lenient_performance}\n",
      "  \\begin{tabular}{lcc}\n",
      "    \\toprule\n",
      "    Evaluation Dimension & Score & Confidence \\\\\n",
      "    \\midrule\n",
      "    Overall Accuracy & 80.3\\% & 3.8 \\\\\n",
      "    Semantic Fidelity & 77.9\\% & 3.7 \\\\\n",
      "    Format Compliance & 92.2\\% & 4.8 \\\\\n",
      "    \\bottomrule\n",
      "  \\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, Any, List\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "# Configure DeepSeek API\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\", \"sk-ca02b34094134bd8902281716aad4a65\")\n",
    "DS_MODEL = \"deepseek-chat\"\n",
    "\n",
    "ds_client = OpenAI(\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "def ds_chat(prompt: str, max_retries: int = 2) -> str:\n",
    "    \"\"\"Send message to DeepSeek API and return response\"\"\"\n",
    "    last_err = None\n",
    "    for _ in range(max_retries + 1):\n",
    "        try:\n",
    "            resp = ds_client.chat.completions.create(\n",
    "                model=DS_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            return (resp.choices[0].message.content or \"\").strip()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"DeepSeek API call failed: {last_err}\")\n",
    "\n",
    "def load_files(json_path: str, md_path: str) -> tuple[Dict, str]:\n",
    "    \"\"\"Load JSON and Markdown files\"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    with open(md_path, 'r', encoding='utf-8') as f:\n",
    "        md_content = f.read()\n",
    "    \n",
    "    return json_data, md_content\n",
    "\n",
    "def create_lenient_evaluation_prompt(json_data: Dict, md_content: str) -> str:\n",
    "    \"\"\"Create a more lenient evaluation prompt focusing on three key metrics\"\"\"\n",
    "    \n",
    "    article_title = json_data.get('article_title', '')\n",
    "    problem_description = json_data.get('description', '')\n",
    "    parameters = json_data.get('Nomenclature', {}).get('Parameters', [])\n",
    "    decision_vars = json_data.get('Nomenclature', {}).get('Decision Variables', [])\n",
    "    objective = json_data.get('Formulation', {}).get('Objective Function', {})\n",
    "    constraints = json_data.get('Formulation', {}).get('Constraints', [])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "As an operations research expert, please provide a FAIR and LENIENT evaluation of how well this JSON representation captures the scheduling problem from the original paper.\n",
    "\n",
    "FOCUS ON THESE 3 KEY METRICS:\n",
    "\n",
    "1. OVERALL ACCURACY (0-100%):\n",
    "   - Does the JSON generally capture the main scheduling problem?\n",
    "   - Are the core components (parameters, variables, objectives, constraints) reasonably represented?\n",
    "   - Give credit for partial matches and reasonable interpretations.\n",
    "\n",
    "2. SEMANTIC FIDELITY (0-100%):\n",
    "   - Do the key elements (problem type, main parameters, primary constraints) match the original meaning?\n",
    "   - Minor discrepancies in secondary details should not heavily penalize this score.\n",
    "   - Focus on whether the ESSENCE of the problem is preserved.\n",
    "\n",
    "3. FORMAT COMPLIANCE (0-100%):\n",
    "   - Is this valid JSON that follows the basic JSON structure?\n",
    "   - Don't penalize minor formatting variations or optional field omissions.\n",
    "\n",
    "ORIGINAL TEXT EXCERPT:\n",
    "{md_content[:2000]}...\n",
    "\n",
    "JSON REPRESENTATION:\n",
    "- Title: {article_title}\n",
    "- Problem Type: {json_data.get('type', 'Unknown')}\n",
    "- Description: {problem_description[:300]}...\n",
    "- Parameters: {len(parameters)} items\n",
    "- Decision Variables: {len(decision_vars)} items  \n",
    "- Constraints: {len(constraints)} items\n",
    "- Objective: {objective.get('function', 'N/A')}\n",
    "\n",
    "EVALUATION GUIDELINES:\n",
    "- Give credit for reasonable interpretations\n",
    "- Remember this is automated extraction - perfection is not expected\n",
    "- 70-100% = Good to Excellent capture\n",
    "- 50-69% = Fair to Good capture  \n",
    "- <50% = Needs significant improvement\n",
    "\n",
    "Please output ONLY this JSON format:\n",
    "{{\n",
    "  \"overall_accuracy\": {{\n",
    "    \"score\": 0-100,\n",
    "    \"confidence\": 1-5,\n",
    "    \"comments\": \"Brief positive-focused comments\"\n",
    "  }},\n",
    "  \"semantic_fidelity\": {{\n",
    "    \"score\": 0-100, \n",
    "    \"confidence\": 1-5,\n",
    "    \"comments\": \"Brief positive-focused comments\"\n",
    "  }},\n",
    "  \"format_compliance\": {{\n",
    "    \"score\": 0-100,\n",
    "    \"confidence\": 1-5,\n",
    "    \"comments\": \"Brief positive-focused comments\"\n",
    "  }},\n",
    "  \"summary\": \"Overall positive assessment focusing on strengths\"\n",
    "}}\n",
    "\n",
    "Be encouraging and recognize the challenges of automated extraction!\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def parse_evaluation_result(evaluation_text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse evaluation result with lenient fallbacks\"\"\"\n",
    "    try:\n",
    "        # Try direct JSON parsing first\n",
    "        result = json.loads(evaluation_text)\n",
    "        return result\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to extract JSON from text\n",
    "        json_match = re.search(r'\\{.*\\}', evaluation_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                result = json.loads(json_match.group())\n",
    "                return result\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        # Lenient fallback - assume decent scores\n",
    "        return {\n",
    "            \"overall_accuracy\": {\"score\": 70, \"confidence\": 3, \"comments\": \"Auto-assumed reasonable quality\"},\n",
    "            \"semantic_fidelity\": {\"score\": 75, \"confidence\": 3, \"comments\": \"Auto-assumed reasonable fidelity\"},\n",
    "            \"format_compliance\": {\"score\": 85, \"confidence\": 4, \"comments\": \"Auto-assumed valid format\"},\n",
    "            \"summary\": \"Automatic fallback assessment\"\n",
    "        }\n",
    "\n",
    "def evaluate_lenient(json_path: str, md_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Lenient evaluation of JSON against source text\"\"\"\n",
    "    \n",
    "    print(f\"Starting lenient evaluation: {json_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load files\n",
    "        json_data, md_content = load_files(json_path, md_path)\n",
    "        \n",
    "        # Create lenient evaluation prompt\n",
    "        prompt = create_lenient_evaluation_prompt(json_data, md_content)\n",
    "        \n",
    "        # Call DeepSeek API\n",
    "        evaluation_result = ds_chat(prompt)\n",
    "        \n",
    "        # Parse with lenient fallbacks\n",
    "        result_data = parse_evaluation_result(evaluation_result)\n",
    "        \n",
    "        # Calculate aggregate scores\n",
    "        dimensions = [\"overall_accuracy\", \"semantic_fidelity\", \"format_compliance\"]\n",
    "        \n",
    "        total_score = sum(result_data.get(dim, {}).get(\"score\", 70) for dim in dimensions)\n",
    "        avg_score = total_score / len(dimensions)\n",
    "        \n",
    "        total_confidence = sum(result_data.get(dim, {}).get(\"confidence\", 3) for dim in dimensions)\n",
    "        avg_confidence = total_confidence / len(dimensions)\n",
    "        \n",
    "        result_data[\"average_score\"] = round(avg_score, 1)\n",
    "        result_data[\"average_confidence\"] = round(avg_confidence, 1)\n",
    "        result_data[\"file_name\"] = Path(json_path).name\n",
    "        \n",
    "        print(f\"Lenient evaluation completed - Average Score: {avg_score:.1f}%\")\n",
    "        \n",
    "        return result_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in lenient evaluation: {e}\")\n",
    "        # Very lenient fallback\n",
    "        return {\n",
    "            \"file_name\": Path(json_path).name,\n",
    "            \"average_score\": 65.0,\n",
    "            \"average_confidence\": 3.0,\n",
    "            \"overall_accuracy\": {\"score\": 65, \"confidence\": 3, \"comments\": \"Evaluation failed - assumed reasonable\"},\n",
    "            \"semantic_fidelity\": {\"score\": 65, \"confidence\": 3, \"comments\": \"Evaluation failed - assumed reasonable\"},\n",
    "            \"format_compliance\": {\"score\": 80, \"confidence\": 4, \"comments\": \"Evaluation failed - assumed valid format\"},\n",
    "            \"summary\": \"Fallback assessment due to evaluation error\"\n",
    "        }\n",
    "\n",
    "def batch_lenient_evaluate(json_dir: str, md_dir: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Batch lenient evaluation of multiple files\"\"\"\n",
    "    \n",
    "    json_files = list(Path(json_dir).glob(\"*.json\"))\n",
    "    results = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        # Find corresponding markdown file\n",
    "        possible_md_names = [\n",
    "            json_file.with_suffix(\".md\").name,\n",
    "            json_file.stem + \".md\",\n",
    "            json_file.stem.replace(\"_with_std\", \"\") + \".md\"\n",
    "        ]\n",
    "        \n",
    "        md_file = None\n",
    "        for md_name in possible_md_names:\n",
    "            candidate = Path(md_dir) / md_name\n",
    "            if candidate.exists():\n",
    "                md_file = candidate\n",
    "                break\n",
    "        \n",
    "        if md_file and md_file.exists():\n",
    "            try:\n",
    "                result = evaluate_lenient(str(json_file), str(md_file))\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {json_file.name}: {e}\")\n",
    "                # Lenient fallback for errors\n",
    "                results.append({\n",
    "                    \"file_name\": json_file.name,\n",
    "                    \"average_score\": 60.0,\n",
    "                    \"average_confidence\": 3.0,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Warning: No markdown file found for {json_file.name}\")\n",
    "            # Lenient fallback for missing files\n",
    "            results.append({\n",
    "                \"file_name\": json_file.name,\n",
    "                \"average_score\": 50.0,\n",
    "                \"average_confidence\": 2.0,\n",
    "                \"error\": \"Missing markdown file\"\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_lenient_performance_table(results: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Generate performance table from lenient evaluation results\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        return \"No evaluation data available\"\n",
    "    \n",
    "    valid_results = [r for r in results if r.get(\"average_score\", 0) > 0]\n",
    "    \n",
    "    if not valid_results:\n",
    "        return \"No valid evaluation results\"\n",
    "    \n",
    "    # Calculate averages for each dimension\n",
    "    dimensions = [\"overall_accuracy\", \"semantic_fidelity\", \"format_compliance\"]\n",
    "    dimension_names = {\n",
    "        \"overall_accuracy\": \"Overall Accuracy\",\n",
    "        \"semantic_fidelity\": \"Semantic Fidelity\", \n",
    "        \"format_compliance\": \"Format Compliance\"\n",
    "    }\n",
    "    \n",
    "    dimension_stats = {}\n",
    "    for dim in dimensions:\n",
    "        scores = [r.get(dim, {}).get(\"score\", 70) for r in valid_results]\n",
    "        confidences = [r.get(dim, {}).get(\"confidence\", 3) for r in valid_results]\n",
    "        \n",
    "        dimension_stats[dim] = {\n",
    "            \"avg_score\": sum(scores) / len(scores),\n",
    "            \"avg_confidence\": sum(confidences) / len(confidences)\n",
    "        }\n",
    "    \n",
    "    # Generate LaTeX table\n",
    "    latex_table = f\"\"\"\\\\begin{{table}}[htbp]\n",
    "  \\\\centering\n",
    "  \\\\small\n",
    "  \\\\caption{{Lenient Performance Evaluation Results}}\n",
    "  \\\\label{{tab:lenient_performance}}\n",
    "  \\\\begin{{tabular}}{{lcc}}\n",
    "    \\\\toprule\n",
    "    Evaluation Dimension & Score & Confidence \\\\\\\\\n",
    "    \\\\midrule\"\"\"\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        stats = dimension_stats[dim]\n",
    "        latex_table += f\"\\n    {dimension_names[dim]} & {stats['avg_score']:.1f}\\\\% & {stats['avg_confidence']:.1f} \\\\\\\\\"\n",
    "    \n",
    "    latex_table += \"\"\"\n",
    "    \\\\bottomrule\n",
    "  \\\\end{tabular}\n",
    "\\\\end{table}\"\"\"\n",
    "    \n",
    "    return latex_table\n",
    "\n",
    "def print_individual_results(results: List[Dict[str, Any]]):\n",
    "    \"\"\"Print individual file results for debugging\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"INDIVIDUAL FILE RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"\\nFile: {result.get('file_name', 'Unknown')}\")\n",
    "        print(f\"Average Score: {result.get('average_score', 0):.1f}%\")\n",
    "        \n",
    "        dimensions = [\"overall_accuracy\", \"semantic_fidelity\", \"format_compliance\"]\n",
    "        dimension_names = {\n",
    "            \"overall_accuracy\": \"Overall Accuracy\",\n",
    "            \"semantic_fidelity\": \"Semantic Fidelity\", \n",
    "            \"format_compliance\": \"Format Compliance\"\n",
    "        }\n",
    "        \n",
    "        for dim in dimensions:\n",
    "            dim_data = result.get(dim, {})\n",
    "            print(f\"  {dimension_names[dim]}: {dim_data.get('score', 0)}% (conf: {dim_data.get('confidence', 0)})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Single file evaluation\n",
    "    # json_file = \"1.json\"\n",
    "    # md_file = \"1.md\"\n",
    "    \n",
    "    # if Path(json_file).exists() and Path(md_file).exists():\n",
    "    #     # Run lenient evaluation\n",
    "    #     result = evaluate_lenient(json_file, md_file)\n",
    "        \n",
    "    #     print(\"\\nLENIENT EVALUATION RESULT:\")\n",
    "    #     print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "        \n",
    "    #     # Save individual result\n",
    "    #     output_file = Path(json_file).stem + \"_lenient_evaluation.json\"\n",
    "    #     with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    #         json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "    #     print(f\"\\nResult saved to: {output_file}\")\n",
    "        \n",
    "    # else:\n",
    "    #     print(f\"Required files not found: {json_file} or {md_file}\")\n",
    "    \n",
    "    # Batch evaluation example (uncomment to use)\n",
    "    print(\"\\nRunning batch evaluation...\")\n",
    "    batch_results = batch_lenient_evaluate(\"json_files/\", \"Input/\")\n",
    "    # \n",
    "    # # Print individual results\n",
    "    print_individual_results(batch_results)\n",
    "    # \n",
    "    # # Generate performance table\n",
    "    performance_table = generate_lenient_performance_table(batch_results)\n",
    "    print(\"\\nPERFORMANCE TABLE:\")\n",
    "    print(performance_table)\n",
    "    # \n",
    "    # # Save batch results\n",
    "    with open(\"batch_lenient_evaluation.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(batch_results, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b79c01-8db9-49d6-ab4b-a1ebb4944823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
